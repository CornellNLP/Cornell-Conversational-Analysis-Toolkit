{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Surprise With ConvoKit\r\n",
    "=====================\r\n",
    "This notebook provides a demo of how to use the Surprise transformer to compute surprise across a corpus. In this demo, we will use the Surprise transformer to compute Speaker Convo Diversity, a measure of how surprising a speaker's participation in one conversation is compared to their participation in all other conversations. We will then compare the results to those obtained using the actual SpeakerConvoDiversity transformer. We eventually want to use the Surprise transformer within the SpeakerConvoDiversity transformer to reduce redundancy, but for now, this demo serves as a sanity check on the correctness of the Surprise transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\n",
    "import numpy as np\n",
    "from convokit import Corpus, download, Surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load a corpus\n",
    "--------\n",
    "For now, we will use data from the subreddit r/Cornell to demonstrate the functionality of this transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists at C:\\Users\\rgang\\.convokit\\downloads\\subreddit-Cornell\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(filename=download('subreddit-Cornell'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 7568\n",
      "Number of Utterances: 74467\n",
      "Number of Conversations: 10744\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up the demo, we will take just the top 100 most active speakers (based on the number of conversations they participate in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_BLACKLIST = ['[deleted]', 'DeltaBot', 'AutoModerator']\n",
    "def utterance_is_valid(utterance):\n",
    "    return utterance.speaker.id not in SPEAKER_BLACKLIST and utterance.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus.organize_speaker_convo_history(utterance_filter=utterance_is_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_activities = corpus.get_attribute_table('speaker', ['n_convos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_convos</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>laveritecestla</th>\n      <td>781.0</td>\n    </tr>\n    <tr>\n      <th>EQUASHNZRKUL</th>\n      <td>726.0</td>\n    </tr>\n    <tr>\n      <th>CornHellUniversity</th>\n      <td>696.0</td>\n    </tr>\n    <tr>\n      <th>t3hasiangod</th>\n      <td>647.0</td>\n    </tr>\n    <tr>\n      <th>ilovemymemesboo</th>\n      <td>430.0</td>\n    </tr>\n    <tr>\n      <th>omgdonerkebab</th>\n      <td>425.0</td>\n    </tr>\n    <tr>\n      <th>cartesiancategory</th>\n      <td>341.0</td>\n    </tr>\n    <tr>\n      <th>cornell256</th>\n      <td>330.0</td>\n    </tr>\n    <tr>\n      <th>mushiettake</th>\n      <td>321.0</td>\n    </tr>\n    <tr>\n      <th>Fencerman2</th>\n      <td>298.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    n_convos\nid                          \nlaveritecestla         781.0\nEQUASHNZRKUL           726.0\nCornHellUniversity     696.0\nt3hasiangod            647.0\nilovemymemesboo        430.0\nomgdonerkebab          425.0\ncartesiancategory      341.0\ncornell256             330.0\nmushiettake            321.0\nFencerman2             298.0"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_activities.sort_values('n_convos', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_speakers = speaker_activities.sort_values('n_convos', ascending=False).head(100).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "subset_utts = [list(corpus.get_speaker(speaker).iter_utterances()) for speaker in top_speakers]\n",
    "subset_corpus = Corpus(utterances=list(itertools.chain(*subset_utts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 100\n",
      "Number of Utterances: 20700\n",
      "Number of Conversations: 6904\n"
     ]
    }
   ],
   "source": [
    "subset_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create instance of Surprise transformer\r\n",
    "---------------\r\n",
    "`target_sample_size` and `context_sample_size` specify the minimum number of tokens that should be in the target and context respectively. If the target or context is too short, the transformer will set the surprise to be `nan`. If we sent these to simply be 1, the most surprising statements tend to just be the very short statements. The transformer takes `n_samples` samples from the target and context transformer (where samples are of size corresponding to `target_sample_size` and `context_sample_size`). It calculates cross entropy for each pair of samples and takes the average to get the final surprise score. This is done to minimize effect of length on scores.\r\n",
    "\r\n",
    "`model_key_selector` defines how utterances in a corpus should be mapped to a model. It takes in an utterance and returns the key for the corresponding model. For this demo we want to map utterances to models based on their speaker and conversation ids.\r\n",
    "\r\n",
    "The transformer also has an optional `cv` to customize the `scikit-learn` `CountVectorizer` used by the transformer to vectorize text. Since we are comparing Surprise to the SpeakerConvoDiversity transformer, we want to make sure that our transformer handles tokenization the same way as SpeakerConvoDiversity, so we will pass in a custom tokenizer function.\r\n",
    "\r\n",
    "The `smooth` parameter determines whether the transformer uses +1 laplace smoothing (`smooth = True`) or naively replaces 0 counts with 1's (`smooth = False`) as SpeakerConvoDiversity does. Here we'll set `smooth = False` since we're comparing the results of Surprise with SpeakerConvoDiversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "import spacy\r\n",
    "spacy_nlp = spacy.load('en', disable=['ner','parser', 'tagger'])\r\n",
    "surp = Surprise(cv=CountVectorizer(tokenizer=lambda x: [t.text for t in spacy_nlp(x)]), model_key_selector=lambda utt: '_'.join([utt.speaker.id, utt.conversation_id]), target_sample_size=100, context_sample_size=100, n_samples=50, smooth=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Fit transformer to corpus\n",
    "-----\n",
    "The `text_func` parameter defines what text each model should be trained on. For this demo, we want a model corresponding to a (speaker, conversation) pair to be trained on all the utterances from the same speaker in different conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "surp = surp.fit(subset_corpus, text_func=lambda utt: [u.text for u in utt.speaker.iter_utterances() if u.conversation_id != utt.conversation_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Transform corpus\n",
    "--------\n",
    "We'll call `transform` with object type `'speaker'` so that surprise scores will be added as a metadata field for each speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [55:38, 33.39s/it]\n"
     ]
    }
   ],
   "source": [
    "transformed_corpus = surp.transform(subset_corpus, 'speaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis\n",
    "------\n",
    "Let's take a look at some of the most surprising speaker conversation involvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "def combine_dicts(x,y):\n",
    "    x.update(y)\n",
    "    return x\n",
    "surprise_scores = reduce(combine_dicts, transformed_corpus.get_speakers_dataframe()['meta.surprise'].values)\n",
    "suprise_series = pd.Series(surprise_scores).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "GROUP_Straight_Derpin_5kst5l__MODEL_Straight_Derpin_5kst5l      4.589240\nGROUP_Dr_Narwhal_6h08sg__MODEL_Dr_Narwhal_6h08sg                4.514626\nGROUP_rrrrrrr1131_8l3xht__MODEL_rrrrrrr1131_8l3xht              4.480983\nGROUP_sasha07974_8v40c1__MODEL_sasha07974_8v40c1                4.477288\nGROUP_ScottVandeberg_8tlcdl__MODEL_ScottVandeberg_8tlcdl        4.472549\nGROUP_t3hasiangod_5v6sqb__MODEL_t3hasiangod_5v6sqb              4.471669\nGROUP_t3hasiangod_4ufm6z__MODEL_t3hasiangod_4ufm6z              4.459614\nGROUP_mushiettake_89mbvs__MODEL_mushiettake_89mbvs              4.458309\nGROUP_SwissWatchesOnly_9hcpip__MODEL_SwissWatchesOnly_9hcpip    4.458051\nGROUP_blackashi_2xxkm4__MODEL_blackashi_2xxkm4                  4.457852\ndtype: float64"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_surprising = suprise_series.sort_values(ascending=False).head(10)\n",
    "most_surprising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some of the least surprising entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "GROUP_shadowclan98_6njs8z__MODEL_shadowclan98_6njs8z        4.203864\nGROUP_dedicateddan_4krfrc__MODEL_dedicateddan_4krfrc        4.210269\nGROUP_Pjcrafty_5apodz__MODEL_Pjcrafty_5apodz                4.211766\nGROUP_dedicateddan_1zcxhv__MODEL_dedicateddan_1zcxhv        4.216961\nGROUP_chaosbutters_9abvcm__MODEL_chaosbutters_9abvcm        4.220616\nGROUP_t3hasiangod_3wtoeo__MODEL_t3hasiangod_3wtoeo          4.224474\nGROUP_shadowclan98_9mfj81__MODEL_shadowclan98_9mfj81        4.227370\nGROUP_laveritecestla_54dxr7__MODEL_laveritecestla_54dxr7    4.228319\nGROUP_CornellMan333_9epekx__MODEL_CornellMan333_9epekx      4.232255\nGROUP_Fencerman2_6tiomd__MODEL_Fencerman2_6tiomd            4.233666\ndtype: float64"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_surprising = suprise_series.sort_values(ascending=True).head(10)\n",
    "least_surprising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to SpeakerConvoDiversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import SpeakerConvoDiversity\r\n",
    "\r\n",
    "scd = SpeakerConvoDiversity('div', select_fn=lambda df, row, aux: (df.convo_id != row.convo_id) & (df.speaker == row.speaker), speaker_cols=['n_convos'], aux_input={'n_iters': 50, 'cmp_sample_size': 100, 'ref_sample_size': 100}, verbosity=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/20700 utterances processed\n",
      "2000/20700 utterances processed\n",
      "3000/20700 utterances processed\n",
      "4000/20700 utterances processed\n",
      "5000/20700 utterances processed\n",
      "6000/20700 utterances processed\n",
      "7000/20700 utterances processed\n",
      "8000/20700 utterances processed\n",
      "9000/20700 utterances processed\n",
      "10000/20700 utterances processed\n",
      "11000/20700 utterances processed\n",
      "12000/20700 utterances processed\n",
      "13000/20700 utterances processed\n",
      "14000/20700 utterances processed\n",
      "15000/20700 utterances processed\n",
      "16000/20700 utterances processed\n",
      "17000/20700 utterances processed\n",
      "18000/20700 utterances processed\n",
      "19000/20700 utterances processed\n",
      "20000/20700 utterances processed\n",
      "20700/20700 utterances processed\n"
     ]
    }
   ],
   "source": [
    "from convokit.text_processing import TextParser\n",
    "\n",
    "tokenizer = TextParser(mode='tokenize', output_field='tokens', verbosity=1000)\n",
    "subset_corpus = tokenizer.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joining tokens across conversation utterances\n",
      "1000 / 15394\n",
      "2000 / 15394\n",
      "3000 / 15394\n",
      "4000 / 15394\n",
      "5000 / 15394\n",
      "6000 / 15394\n",
      "7000 / 15394\n",
      "8000 / 15394\n",
      "9000 / 15394\n",
      "10000 / 15394\n",
      "11000 / 15394\n",
      "12000 / 15394\n",
      "13000 / 15394\n",
      "14000 / 15394\n",
      "15000 / 15394\n"
     ]
    }
   ],
   "source": [
    "div_transformed = scd.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the speaker convo entries that have the highest diversity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>convo_id</th>\n      <th>convo_idx</th>\n      <th>div</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Straight_Derpin__5kst5l</th>\n      <td>Straight_Derpin</td>\n      <td>5kst5l</td>\n      <td>34</td>\n      <td>4.590349</td>\n    </tr>\n    <tr>\n      <th>Dr_Narwhal__6h08sg</th>\n      <td>Dr_Narwhal</td>\n      <td>6h08sg</td>\n      <td>75</td>\n      <td>4.539678</td>\n    </tr>\n    <tr>\n      <th>SwissWatchesOnly__9hcpip</th>\n      <td>SwissWatchesOnly</td>\n      <td>9hcpip</td>\n      <td>129</td>\n      <td>4.500917</td>\n    </tr>\n    <tr>\n      <th>ScottVandeberg__8tlcdl</th>\n      <td>ScottVandeberg</td>\n      <td>8tlcdl</td>\n      <td>81</td>\n      <td>4.499456</td>\n    </tr>\n    <tr>\n      <th>sasha07974__8v40c1</th>\n      <td>sasha07974</td>\n      <td>8v40c1</td>\n      <td>42</td>\n      <td>4.497518</td>\n    </tr>\n    <tr>\n      <th>rrrrrrr1131__8l3xht</th>\n      <td>rrrrrrr1131</td>\n      <td>8l3xht</td>\n      <td>25</td>\n      <td>4.497179</td>\n    </tr>\n    <tr>\n      <th>blackashi__2xxkm4</th>\n      <td>blackashi</td>\n      <td>2xxkm4</td>\n      <td>6</td>\n      <td>4.490293</td>\n    </tr>\n    <tr>\n      <th>agottler__9iyo8u</th>\n      <td>agottler</td>\n      <td>9iyo8u</td>\n      <td>66</td>\n      <td>4.489280</td>\n    </tr>\n    <tr>\n      <th>t3hasiangod__5v6sqb</th>\n      <td>t3hasiangod</td>\n      <td>5v6sqb</td>\n      <td>590</td>\n      <td>4.484694</td>\n    </tr>\n    <tr>\n      <th>EQUASHNZRKUL__82l7qg</th>\n      <td>EQUASHNZRKUL</td>\n      <td>82l7qg</td>\n      <td>328</td>\n      <td>4.471964</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                                   speaker convo_id  convo_idx       div\nid                                                                      \nStraight_Derpin__5kst5l    Straight_Derpin   5kst5l         34  4.590349\nDr_Narwhal__6h08sg              Dr_Narwhal   6h08sg         75  4.539678\nSwissWatchesOnly__9hcpip  SwissWatchesOnly   9hcpip        129  4.500917\nScottVandeberg__8tlcdl      ScottVandeberg   8tlcdl         81  4.499456\nsasha07974__8v40c1              sasha07974   8v40c1         42  4.497518\nrrrrrrr1131__8l3xht            rrrrrrr1131   8l3xht         25  4.497179\nblackashi__2xxkm4                blackashi   2xxkm4          6  4.490293\nagottler__9iyo8u                  agottler   9iyo8u         66  4.489280\nt3hasiangod__5v6sqb            t3hasiangod   5v6sqb        590  4.484694\nEQUASHNZRKUL__82l7qg          EQUASHNZRKUL   82l7qg        328  4.471964"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_transformed.get_speaker_convo_attribute_table(attrs=['div']).sort_values('div', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the diversity scores returned by `SpeakerConvoDiversity` are slightly different from the scores returned by the `Surprise` transformer. This difference can be attributed to the addition of Laplace smoothing in the `Surprise` transformer to account for out of vocabulary tokens. The `SpeakerConvoDiversity` transformer deals with OOV tokens by simply treating their count as 1. If you run the `Surprise` transformer with the `smooth` flag set to false, the transformer will treat OOV tokens the same way `SpeakerConvoDiversity` does. When run without smoothing, the `Surprise` transformer returns the same scores as `SpeakerConvoDiversity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the least diverse speaker-convo entries based on the SpeakerConvoDiversity transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>speaker</th>\n      <th>convo_id</th>\n      <th>convo_idx</th>\n      <th>div</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Fencerman2__6tiomd</th>\n      <td>Fencerman2</td>\n      <td>6tiomd</td>\n      <td>111</td>\n      <td>4.204802</td>\n    </tr>\n    <tr>\n      <th>cornell256__96utv3</th>\n      <td>cornell256</td>\n      <td>96utv3</td>\n      <td>292</td>\n      <td>4.211056</td>\n    </tr>\n    <tr>\n      <th>dedicateddan__4krfrc</th>\n      <td>dedicateddan</td>\n      <td>4krfrc</td>\n      <td>97</td>\n      <td>4.218379</td>\n    </tr>\n    <tr>\n      <th>t3hasiangod__3wtoeo</th>\n      <td>t3hasiangod</td>\n      <td>3wtoeo</td>\n      <td>46</td>\n      <td>4.225599</td>\n    </tr>\n    <tr>\n      <th>t3hasiangod__4ar3u0</th>\n      <td>t3hasiangod</td>\n      <td>4ar3u0</td>\n      <td>99</td>\n      <td>4.230127</td>\n    </tr>\n    <tr>\n      <th>Enyo287__5ipedu</th>\n      <td>Enyo287</td>\n      <td>5ipedu</td>\n      <td>281</td>\n      <td>4.231756</td>\n    </tr>\n    <tr>\n      <th>t3hasiangod__4me3m0</th>\n      <td>t3hasiangod</td>\n      <td>4me3m0</td>\n      <td>168</td>\n      <td>4.233925</td>\n    </tr>\n    <tr>\n      <th>iBeReese__1uuldh</th>\n      <td>iBeReese</td>\n      <td>1uuldh</td>\n      <td>6</td>\n      <td>4.234036</td>\n    </tr>\n    <tr>\n      <th>Pjcrafty__5apodz</th>\n      <td>Pjcrafty</td>\n      <td>5apodz</td>\n      <td>17</td>\n      <td>4.234668</td>\n    </tr>\n    <tr>\n      <th>dedicateddan__1zcxhv</th>\n      <td>dedicateddan</td>\n      <td>1zcxhv</td>\n      <td>20</td>\n      <td>4.239415</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                           speaker convo_id  convo_idx       div\nid                                                              \nFencerman2__6tiomd      Fencerman2   6tiomd        111  4.204802\ncornell256__96utv3      cornell256   96utv3        292  4.211056\ndedicateddan__4krfrc  dedicateddan   4krfrc         97  4.218379\nt3hasiangod__3wtoeo    t3hasiangod   3wtoeo         46  4.225599\nt3hasiangod__4ar3u0    t3hasiangod   4ar3u0         99  4.230127\nEnyo287__5ipedu            Enyo287   5ipedu        281  4.231756\nt3hasiangod__4me3m0    t3hasiangod   4me3m0        168  4.233925\niBeReese__1uuldh          iBeReese   1uuldh          6  4.234036\nPjcrafty__5apodz          Pjcrafty   5apodz         17  4.234668\ndedicateddan__1zcxhv  dedicateddan   1zcxhv         20  4.239415"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "div_transformed.get_speaker_convo_attribute_table(attrs=['div']).sort_values('div').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprise With Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [43:02, 25.83s/it]\n"
     ]
    }
   ],
   "source": [
    "surp = Surprise(cv=CountVectorizer(tokenizer=lambda x: [t.text for t in spacy_nlp(x)]), model_key_selector=lambda utt: '_'.join([utt.speaker.id, utt.conversation_id]), surprise_attr_name='surprise_smoothed', target_sample_size=100, context_sample_size=100, n_samples=50, smooth=True)\r\n",
    "surp.fit(subset_corpus, text_func=lambda utt: [u.text for u in utt.speaker.iter_utterances() if u.conversation_id != utt.conversation_id])\r\n",
    "transformed_corpus = surp.transform(subset_corpus, 'speaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "from functools import reduce\r\n",
    "def combine_dicts(x,y):\r\n",
    "    x.update(y)\r\n",
    "    return x\r\n",
    "surprise_scores = reduce(combine_dicts, transformed_corpus.get_speakers_dataframe()['meta.surprise'].values)\r\n",
    "suprise_series = pd.Series(surprise_scores).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "GROUP_Straight_Derpin_5kst5l__MODEL_Straight_Derpin_5kst5l      4.589240\nGROUP_Dr_Narwhal_6h08sg__MODEL_Dr_Narwhal_6h08sg                4.514626\nGROUP_rrrrrrr1131_8l3xht__MODEL_rrrrrrr1131_8l3xht              4.480983\nGROUP_sasha07974_8v40c1__MODEL_sasha07974_8v40c1                4.477288\nGROUP_ScottVandeberg_8tlcdl__MODEL_ScottVandeberg_8tlcdl        4.472549\nGROUP_t3hasiangod_5v6sqb__MODEL_t3hasiangod_5v6sqb              4.471669\nGROUP_t3hasiangod_4ufm6z__MODEL_t3hasiangod_4ufm6z              4.459614\nGROUP_mushiettake_89mbvs__MODEL_mushiettake_89mbvs              4.458309\nGROUP_SwissWatchesOnly_9hcpip__MODEL_SwissWatchesOnly_9hcpip    4.458051\nGROUP_blackashi_2xxkm4__MODEL_blackashi_2xxkm4                  4.457852\ndtype: float64"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_surprising = suprise_series.sort_values(ascending=False).head(10)\r\n",
    "most_surprising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "GROUP_shadowclan98_6njs8z__MODEL_shadowclan98_6njs8z        4.203864\nGROUP_dedicateddan_4krfrc__MODEL_dedicateddan_4krfrc        4.210269\nGROUP_Pjcrafty_5apodz__MODEL_Pjcrafty_5apodz                4.211766\nGROUP_dedicateddan_1zcxhv__MODEL_dedicateddan_1zcxhv        4.216961\nGROUP_chaosbutters_9abvcm__MODEL_chaosbutters_9abvcm        4.220616\nGROUP_t3hasiangod_3wtoeo__MODEL_t3hasiangod_3wtoeo          4.224474\nGROUP_shadowclan98_9mfj81__MODEL_shadowclan98_9mfj81        4.227370\nGROUP_laveritecestla_54dxr7__MODEL_laveritecestla_54dxr7    4.228319\nGROUP_CornellMan333_9epekx__MODEL_CornellMan333_9epekx      4.232255\nGROUP_Fencerman2_6tiomd__MODEL_Fencerman2_6tiomd            4.233666\ndtype: float64"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "least_surprising = suprise_series.sort_values(ascending=True).head(10)\r\n",
    "least_surprising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convokit-venv",
   "language": "python",
   "name": "convokit-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}