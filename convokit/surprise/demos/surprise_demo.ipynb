{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing Surprise With ConvoKit\n",
    "=====================\n",
    "This notebook provides a demo of how to use the Surprise transformer to compute surprise across a corpus. In this demo, we will use the Surprise transformer to compute Speaker Convo Diversity, a measure of how surprising a speaker's participation in one conversation is compared to their participation in all other conversations. We will then compare the results to those obtained using the actual SpeakerConvoDiversity transformer. We eventually want to use the Surprise transformer within the SpeakerConvoDiversity transformer to reduce redundancy, but for now, this demo serves as a sanity check on the correctness of the Surprise transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import convokit\r\n",
    "import numpy as np\r\n",
    "from convokit import Corpus, download, Surprise\r\n",
    "from convokit.text_processing import TextProcessor, TextParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load a corpus\n",
    "--------\n",
    "For now, we will use data from the subreddit r/Cornell to demonstrate the functionality of this transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus = Corpus('C:\\\\Users\\\\rgang\\\\.convokit\\\\downloads\\\\subreddit-Cornell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 7568\n",
      "Number of Utterances: 74467\n",
      "Number of Conversations: 10744\n"
     ]
    }
   ],
   "source": [
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to speed up the demo, we will take just the top 100 most active speakers (based on the number of conversations they participate in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_BLACKLIST = ['[deleted]', 'DeltaBot', 'AutoModerator']\n",
    "def utterance_is_valid(utterance):\n",
    "    return utterance.speaker.id not in SPEAKER_BLACKLIST and utterance.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\rgang\\documents\\cornell\\research\\cornell-conversational-analysis-toolkit\\convokit\\model\\corpus.py:1213: FutureWarning: set_info() is deprecated and will be removed in a future release. Use add_meta() instead.\n",
      "c:\\users\\rgang\\documents\\cornell\\research\\cornell-conversational-analysis-toolkit\\convokit\\model\\corpus.py:1219: FutureWarning: set_info() is deprecated and will be removed in a future release. Use add_meta() instead.\n"
     ]
    }
   ],
   "source": [
    "corpus.organize_speaker_convo_history(utterance_filter=utterance_is_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_activities = corpus.get_attribute_table('speaker', ['n_convos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>n_convos</th>\n    </tr>\n    <tr>\n      <th>id</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>laveritecestla</th>\n      <td>781.0</td>\n    </tr>\n    <tr>\n      <th>EQUASHNZRKUL</th>\n      <td>726.0</td>\n    </tr>\n    <tr>\n      <th>CornHellUniversity</th>\n      <td>696.0</td>\n    </tr>\n    <tr>\n      <th>t3hasiangod</th>\n      <td>647.0</td>\n    </tr>\n    <tr>\n      <th>ilovemymemesboo</th>\n      <td>430.0</td>\n    </tr>\n    <tr>\n      <th>omgdonerkebab</th>\n      <td>425.0</td>\n    </tr>\n    <tr>\n      <th>cartesiancategory</th>\n      <td>341.0</td>\n    </tr>\n    <tr>\n      <th>cornell256</th>\n      <td>330.0</td>\n    </tr>\n    <tr>\n      <th>mushiettake</th>\n      <td>321.0</td>\n    </tr>\n    <tr>\n      <th>Fencerman2</th>\n      <td>298.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "                    n_convos\nid                          \nlaveritecestla         781.0\nEQUASHNZRKUL           726.0\nCornHellUniversity     696.0\nt3hasiangod            647.0\nilovemymemesboo        430.0\nomgdonerkebab          425.0\ncartesiancategory      341.0\ncornell256             330.0\nmushiettake            321.0\nFencerman2             298.0"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_activities.sort_values('n_convos', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_speakers = speaker_activities.sort_values('n_convos', ascending=False).head(25).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\r\n",
    "\r\n",
    "subset_utts = [list(corpus.get_speaker(speaker).iter_utterances(selector=utterance_is_valid)) for speaker in top_speakers]\r\n",
    "subset_corpus = Corpus(utterances=list(itertools.chain(*subset_utts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Speakers: 25\n",
      "Number of Utterances: 11145\n",
      "Number of Conversations: 5082\n"
     ]
    }
   ],
   "source": [
    "subset_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/11145 utterances processed\n",
      "2000/11145 utterances processed\n",
      "3000/11145 utterances processed\n",
      "4000/11145 utterances processed\n",
      "5000/11145 utterances processed\n",
      "6000/11145 utterances processed\n",
      "7000/11145 utterances processed\n",
      "8000/11145 utterances processed\n",
      "9000/11145 utterances processed\n",
      "10000/11145 utterances processed\n",
      "11000/11145 utterances processed\n",
      "11145/11145 utterances processed\n"
     ]
    },
    {
     "data": {
      "text/plain": "<convokit.model.corpus.Corpus at 0x2375df2ac08>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from convokit.text_processing import TextParser\n",
    "from convokit.speaker_convo_helpers.speaker_convo_attrs import SpeakerConvoAttrs\n",
    "\n",
    "tokenizer = TextParser(mode='tokenize', output_field='tokens', verbosity=1000)\n",
    "subset_corpus = tokenizer.transform(subset_corpus)\n",
    "\n",
    "def _join_all_tokens(parses):\n",
    "    joined = []\n",
    "    for parse in parses:\n",
    "        for sent in parse:\n",
    "            joined += [tok['tok'].lower() for tok in sent['toks']]\n",
    "    return joined\n",
    "\n",
    "agg_tokens = SpeakerConvoAttrs('tokens',\n",
    "                 agg_fn=_join_all_tokens,\n",
    "                 recompute=False)\n",
    "\n",
    "agg_tokens.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Create instance of Surprise transformer\n",
    "---------------\n",
    "`target_sample_size` and `context_sample_size` specify the minimum number of tokens that should be in the target and context respectively. If the target or context is too short, the transformer will set the surprise to be `nan`. If we sent these to simply be 1, the most surprising statements tend to just be the very short statements. The transformer takes `n_samples` samples from the target and context transformer (where samples are of size corresponding to `target_sample_size` and `context_sample_size`). It calculates cross entropy for each pair of samples and takes the average to get the final surprise score. This is done to minimize effect of length on scores.\n",
    "\n",
    "`model_key_selector` defines how utterances in a corpus should be mapped to a model. It takes in an utterance and returns the key for the corresponding model. For this demo we want to map utterances to models based on their speaker and conversation ids.\n",
    "\n",
    "The transformer also has an optional `cv` to customize the `scikit-learn` `CountVectorizer` used by the transformer to vectorize text. Since we are comparing Surprise to the SpeakerConvoDiversity transformer, we want to make sure that our transformer handles tokenization the same way as SpeakerConvoDiversity, so we will pass in a custom tokenizer function.\n",
    "\n",
    "The `smooth` parameter determines whether the transformer uses +1 laplace smoothing (`smooth = True`) or naively replaces 0 counts with 1's (`smooth = False`) as SpeakerConvoDiversity does. Here we'll set `smooth = False` since we're comparing the results of Surprise with SpeakerConvoDiversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "surp = Surprise(cv=CountVectorizer(preprocessor=lambda x: x, tokenizer=lambda x: x), model_key_selector=lambda utt: '_'.join([utt.speaker.id, utt.conversation_id]), target_sample_size=100, context_sample_size=100, n_samples=50, smooth=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Fit transformer to corpus\n",
    "-----\n",
    "The `text_func` parameter defines what text each model should be trained on. For this demo, we want a model corresponding to a (speaker, conversation) pair to be trained on all the utterances from the same speaker in different conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3269it [01:28, 16.89it/s]"
     ]
    }
   ],
   "source": [
    "from itertools import chain\r\n",
    "speaker_convo_attr_table = subset_corpus.get_full_attribute_table(['tokens'])\r\n",
    "\r\n",
    "def _get_text_func(utt, df):\r\n",
    "  utt_row = df.loc[f'{utt.speaker.id}__{utt.conversation_id}']\r\n",
    "  ref_subset = df[(df.convo_idx % 2 != utt_row.convo_idx % 2) & (df.speaker == utt_row.speaker)]\r\n",
    "  return [np.array(list(chain(*ref_subset.tokens.values)))]\r\n",
    "\r\n",
    "surp = surp.fit(subset_corpus, text_func=lambda utt: _get_text_func(utt, speaker_convo_attr_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Transform corpus\n",
    "--------\n",
    "We'll call `transform` with object type `'speaker'` so that surprise scores will be added as a metadata field for each speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]c:\\users\\rgang\\documents\\cornell\\research\\cornell-conversational-analysis-toolkit\\venv\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of unsized object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-b7bd2d33c481>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtransformed_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msurp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset_corpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'speaker'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\rgang\\documents\\cornell\\research\\cornell-conversational-analysis-toolkit\\convokit\\surprise\\surprise.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, corpus, obj_type, group_and_models, selector, target_text_func)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_key\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mutt_groups\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgroup_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[0msurprise_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSurprise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_format_attr_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_surprise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_meta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurprise_attr_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msurprise_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rgang\\documents\\cornell\\research\\cornell-conversational-analysis-toolkit\\convokit\\surprise\\surprise.py\u001b[0m in \u001b[0;36m_compute_surprise\u001b[1;34m(self, target, context)\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[0mcontext_tokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[0mtarget_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtarget_tokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_sample_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 221\u001b[1;33m     \u001b[0mcontext_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msampling_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext_sample_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtarget_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcontext_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rgang\\documents\\cornell\\research\\cornell-conversational-analysis-toolkit\\convokit\\surprise\\surprise.py\u001b[0m in \u001b[0;36msample\u001b[1;34m(tokens, sample_size, n_samples, p)\u001b[0m\n\u001b[0;32m     36\u001b[0m   \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0meach\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msample\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m   \"\"\"\n\u001b[1;32m---> 38\u001b[1;33m   \u001b[0mtokens_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoks\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0msample_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtokens_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[0mrng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rgang\\documents\\cornell\\research\\cornell-conversational-analysis-toolkit\\convokit\\surprise\\surprise.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m   \u001b[1;33m:\u001b[0m\u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0meach\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0msample\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m   \"\"\"\n\u001b[1;32m---> 38\u001b[1;33m   \u001b[0mtokens_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoks\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoks\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoks\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0msample_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtokens_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[0mrng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: len() of unsized object"
     ]
    }
   ],
   "source": [
    "transformed_corpus = surp.transform(subset_corpus, 'speaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis\n",
    "------\n",
    "Let's take a look at some of the most surprising speaker conversation involvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "def combine_dicts(x,y):\n",
    "    x.update(y)\n",
    "    return x\n",
    "surprise_scores = reduce(combine_dicts, transformed_corpus.get_speakers_dataframe()['meta.surprise'].values)\n",
    "suprise_series = pd.Series(surprise_scores).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_surprising = suprise_series.sort_values(ascending=False).head(10)\n",
    "most_surprising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at some of the least surprising entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_surprising = suprise_series.sort_values(ascending=True).head(10)\n",
    "least_surprising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to SpeakerConvoDiversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import SpeakerConvoDiversity\n",
    "\n",
    "scd = SpeakerConvoDiversity('div', select_fn=lambda df, row, aux: (df.convo_id != row.convo_id) & (df.speaker == row.speaker), speaker_cols=['n_convos'], aux_input={'n_iters': 50, 'cmp_sample_size': 100, 'ref_sample_size': 100}, verbosity=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "div_transformed = scd.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the speaker convo entries that have the highest diversity score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_transformed.get_speaker_convo_attribute_table(attrs=['div']).sort_values('div', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the diversity scores returned by `SpeakerConvoDiversity` are slightly different from the scores returned by the `Surprise` transformer. This difference can be attributed to the addition of Laplace smoothing in the `Surprise` transformer to account for out of vocabulary tokens. The `SpeakerConvoDiversity` transformer deals with OOV tokens by simply treating their count as 1. If you run the `Surprise` transformer with the `smooth` flag set to false, the transformer will treat OOV tokens the same way `SpeakerConvoDiversity` does. When run without smoothing, the `Surprise` transformer returns the same scores as `SpeakerConvoDiversity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the least diverse speaker-convo entries based on the SpeakerConvoDiversity transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "div_transformed.get_speaker_convo_attribute_table(attrs=['div']).sort_values('div').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Surprise With Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surp = Surprise(cv=CountVectorizer(tokenizer=lambda x: [t.text for t in spacy_nlp(x)]), model_key_selector=lambda utt: '_'.join([utt.speaker.id, utt.conversation_id]), surprise_attr_name='surprise_smoothed', target_sample_size=100, context_sample_size=100, n_samples=50, smooth=True)\n",
    "surp.fit(subset_corpus, text_func=lambda utt: [u.text for u in utt.speaker.iter_utterances() if u.conversation_id != utt.conversation_id])\n",
    "transformed_corpus = surp.transform(subset_corpus, 'speaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from functools import reduce\n",
    "def combine_dicts(x,y):\n",
    "    x.update(y)\n",
    "    return x\n",
    "surprise_scores = reduce(combine_dicts, transformed_corpus.get_speakers_dataframe()['meta.surprise_smoothed'].values)\n",
    "suprise_series = pd.Series(surprise_scores).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_surprising = suprise_series.sort_values(ascending=False).head(10)\n",
    "most_surprising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "least_surprising = suprise_series.sort_values(ascending=True).head(10)\n",
    "least_surprising"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpeakerConvoDiversity reimplemented using Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import SpeakerConvoDiversityWrapper\n",
    "from convokit.speakerConvoDiversity.speakerConvoDiversity2 import SpeakerConvoDiversityWrapper as SpeakerConvoDiversityWrapper2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus('C:\\\\Users\\\\rgang\\\\.convokit\\\\downloads\\\\subreddit-Cornell')\n",
    "corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEAKER_BLACKLIST = ['[deleted]', 'DeltaBot','AutoModerator']\n",
    "def utterance_is_valid(utterance):\n",
    "    return (utterance.id != utterance.conversation_id) and (utterance.speaker.id not in SPEAKER_BLACKLIST)\n",
    "\n",
    "corpus.organize_speaker_convo_history(utterance_filter=utterance_is_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_activities = corpus.get_attribute_table('speaker',['n_convos'])\n",
    "top_speakers = speaker_activities.sort_values('n_convos', ascending=False).head(25).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_utts = []\n",
    "for speaker in top_speakers:\n",
    "    subset_utts += list(corpus.get_speaker(speaker).iter_utterances(selector=utterance_is_valid))\n",
    "subset_corpus = Corpus(utterances=subset_utts)\n",
    "subset_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TextParser(mode='tokenize', output_field='tokens', verbosity=1000)\n",
    "subset_corpus = tokenizer.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scd = SpeakerConvoDiversityWrapper(lifestage_size=2, max_exp=20,\n",
    "                sample_size=20, min_n_utterances=1, n_iters=50, cohort_delta=60*60*24*30*2, verbosity=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus = scd.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus.get_speaker_convo_attribute_table(attrs=['div__self', 'div__other', 'div__adj']).dropna(subset=['div__self', 'div__other', 'div__adj'], how='all').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus = Corpus(utterances=subset_utts)\n",
    "subset_corpus.print_summary_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scd = SpeakerConvoDiversityWrapper2(lifestage_size=2, max_exp=20,\n",
    "                sample_size=20, min_n_utterances=1, n_iters=50, cohort_delta=60*60*24*30*2, verbosity=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus = scd.transform(subset_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_corpus.get_speaker_convo_attribute_table(attrs=['div__self', 'div__other', 'div__adj']).dropna(subset=['div__self', 'div__other', 'div__adj'], how='all').head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "convokit-venv",
   "language": "python",
   "name": "convokit-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}